Using 1 GPU(s).
logp_net
Wide_ResNet(
  (lrelu): LeakyReLU(negative_slope=0.2)
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (layer1): Sequential(
    (0): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential(
        (0): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (1): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (2): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (3): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2))
      )
    )
    (1): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (2): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (3): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (shortcut): Sequential(
        (0): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2))
      )
    )
    (1): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (2): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (3): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
  )
  (bn1): Identity()
  (linear): Linear(in_features=640, out_features=10, bias=True)
)
generator
VERAGenerator(
  (g): ResNetGenerator(
    (input_linear): Linear(in_features=128, out_features=4096, bias=True)
    (block1): GeneratorBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
    )
    (block2): GeneratorBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
    )
    (block3): GeneratorBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
    )
    (output_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (output_conv): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu): ReLU()
  )
)
36471338 ebm parameters
4079492 generator parameters
Using 1 GPU(s).
logp_net
Wide_ResNet(
  (lrelu): LeakyReLU(negative_slope=0.2)
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (layer1): Sequential(
    (0): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential(
        (0): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (1): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (2): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (3): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2))
      )
    )
    (1): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (2): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (3): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (shortcut): Sequential(
        (0): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2))
      )
    )
    (1): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (2): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (3): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
  )
  (bn1): Identity()
  (linear): Linear(in_features=640, out_features=10, bias=True)
)
generator
VERAGenerator(
  (g): ResNetGenerator(
    (input_linear): Linear(in_features=128, out_features=4096, bias=True)
    (block1): GeneratorBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
    )
    (block2): GeneratorBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
    )
    (block3): GeneratorBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
    )
    (output_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (output_conv): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu): ReLU()
  )
)
36471338 ebm parameters
4079492 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
12603435 ebm parameters
12603564 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
12603435 ebm parameters
12603564 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
12603819 ebm parameters
12603948 generator parameters
Using 1 GPU(s).
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
  )
)
12622635 ebm parameters
12622764 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
  )
)
12622635 ebm parameters
12622764 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
  )
)
12622635 ebm parameters
12622764 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
  )
)
12622635 ebm parameters
12622764 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
  )
)
12622635 ebm parameters
12622764 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Wide_ResNet(
  (lrelu): LeakyReLU(negative_slope=0.2)
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (layer1): Sequential(
    (0): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential(
        (0): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (1): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (2): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (3): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2))
      )
    )
    (1): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (2): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (3): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (shortcut): Sequential(
        (0): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2))
      )
    )
    (1): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (2): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
    (3): wide_basic(
      (lrelu): LeakyReLU(negative_slope=0.2)
      (bn1): Identity()
      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (dropout): Dropout(p=0.3, inplace=False)
      (bn2): Identity()
      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (shortcut): Sequential()
    )
  )
  (bn1): Identity()
  (linear): Linear(in_features=640, out_features=10, bias=True)
)
generator
VERAGenerator(
  (g): ResNetGenerator(
    (input_linear): Linear(in_features=128, out_features=4096, bias=True)
    (block1): GeneratorBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
    )
    (block2): GeneratorBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
    )
    (block3): GeneratorBlock(
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
    )
    (output_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (output_conv): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu): ReLU()
  )
)
36471338 ebm parameters
4079492 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12623139 ebm parameters
12623268 generator parameters
Using 1 GPU(s).
logp_net
Generator(
  (mlp): Linear(in_features=128, out_features=24576, bias=True)
  (TransformerEncoder_encoder1): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Encoder_Block(
        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=384, out_features=384, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder2): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Encoder_Block(
        (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=96, out_features=288, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=96, out_features=96, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (TransformerEncoder_encoder3): TransformerEncoder(
    (Encoder_Blocks): ModuleList(
      (0): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Encoder_Block(
        (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=24, out_features=72, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
          (out): Sequential(
            (0): Linear(in_features=24, out_features=24, bias=True)
            (1): Dropout(p=0.0, inplace=False)
          )
        )
        (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=24, out_features=96, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=96, out_features=24, bias=True)
          (droprateout): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (PatchMerging3): PatchMerging(
    (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging2): PatchMerging(
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (PatchMerging1): PatchMerging(
    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (linear): Sequential(
    (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (patches): ImgPatches(
    (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
  )
  (droprate): Dropout(p=0.0, inplace=False)
)
generator
VERAGenerator(
  (g): Generator(
    (mlp): Linear(in_features=128, out_features=24576, bias=True)
    (TransformerEncoder_encoder1): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Encoder_Block(
          (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=384, out_features=1152, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=384, out_features=384, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder2): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Encoder_Block(
          (ln1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=96, out_features=96, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (TransformerEncoder_encoder3): TransformerEncoder(
      (Encoder_Blocks): ModuleList(
        (0): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Encoder_Block(
          (ln1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=24, out_features=72, bias=False)
            (attention_dropout): Dropout(p=0.0, inplace=False)
            (out): Sequential(
              (0): Linear(in_features=24, out_features=24, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (ln2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=24, out_features=96, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=96, out_features=24, bias=True)
            (droprateout): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (PatchMerging3): PatchMerging(
      (norm): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging2): PatchMerging(
      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
    (PatchMerging1): PatchMerging(
      (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    )
    (linear): Sequential(
      (0): Conv2d(24, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (patches): ImgPatches(
      (patch_embed): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))
    )
    (droprate): Dropout(p=0.0, inplace=False)
  )
)
12624675 ebm parameters
12624804 generator parameters
